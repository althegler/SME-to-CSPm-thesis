\documentclass[a4paper]{report}

\usepackage[latin1]{inputenc}
\usepackage{palatino}
\usepackage[usenames]{color}

\usepackage{hyperref}

\usepackage{chngpage}
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{multirow}

\usepackage[]{algorithm2e}
\usepackage{varwidth}

\usepackage{setspace}
\usepackage{hyperref}

\usepackage{todonotes}



\usepackage[babel, en, nat, farve, titelside]{ku-forside}

% (asbjoern commando: nbuf command)
% \newcommand{\nbuf}{\textit{nbuf} }

\newcommand{\cspm}{CSP$_M$ }


\usepackage{fancyhdr}
\pagestyle{fancy}

\lhead{Alberte Thegler}
\chead{Master's Thesis}
\rhead{August 2018}

\renewcommand{\headrulewidth}{0.4pt} % thickness of line at header
\renewcommand{\footrulewidth}{0.4pt} % thickness of line at footer
\setlength{\belowcaptionskip}{-10pt} % space below captions


\opgave{Master's Thesis}
\title{Towards Automatic Program Specification From SME Models}
\undertitel{Department of Computer Science}
\author{Alberte Thegler - alberte@thegler.dk}
\date{November 2018}
\vejleder{Advisors: Professor Brian Vinter and Kenneth Skovhede}

\begin{document}

\maketitle

\pagenumbering{roman}
\begin{abstract}
\begin{doublespace}
Abstract

\end{doublespace}
\end{abstract}



\newpage
\tableofcontents
%
% Abstract
% acknowledgements
% Introduction
% Motivation
% Background
% Code generation
% Evaluation
% discussion
% conclusion
% bibliography
%
\newpage
%\listoftodos
%\newpage
\pagenumbering{arabic}
\chapter{Introduction}
When we create programs, we wish to verify that it is also correct. There are several ways to do this, one commenly used is \texttt{testing} which require that the programmer creates several different scenarios and its expected output, or that the programmer programs a test-generator to create the scenarios and expected output. This, however, is not adequate for critical systems since it is never a 100\% accurate. Therefore it is of high interest to create a formal verification of the system or program.\\
%Talk about how verification was first created and how it became to be used for concurrent systems. Then write about how it works and then write about the different systems and formal languages that is used for it. \\\\


In this thesis we look at model checking, that is, verifying that a specific property will always hold for a piece of code.
\\\\

Formal verification is the process of checking whether a program satisfies specific properties. Different methods have evolved, all having different advantages and disadvantages. FDR is sometimes referred to as a model checker however is it actually a refinement checker.
\\\\


\textit{"Matematicians tend to reject proofs by exhaustive checking of all cases as being less satisfying than deductive proofs, and with good reason. First, they are not applicable for proving theorems about integers and real numbers, which are infinite domains so that the number of interpretations is infinite and they cannot be exhaustively checked. Second, they offer no insight into why a theorem is true. But computer scientists have more practical concerns. If they can check all computations of a program and show that they all satisfy a correctness property, we will be willing to forego elegance and be more than satisfied that our program has been proven correct."} from "A primer on model checking af Ben-Ari" \cite{Ben-ari2010}\
\newpage
\section{Motivation}
\subsection{Ariane 5 failure}
The Ariane 5 space rocket\cite{InquiryBoard1996} was designed to launch large payloads into Earths orbit, such as communications satelites, etc. Ariane 5 was the follow-up on the sucessful Ariane 4 launchers. On june 4th, 1996, the Ariane 5 rocket had its first test flight. The rocket, which was owned by The European Space Agency (ESA) and the French spatial agency Centre national d'\'etudes spatiales (CNES) was manufactured by Airbus Defence and Space.\\ The rocket was launched in French Guiana, and only 37 seconds after successful lift-off, the rocket flipped 90 degrees and two seconds later the forces of aerodynamics ripped the boosters appart from the core stage. This caused the self-destruct mechanism to trigger and the rocket self-destructed in a giant explotion shortly afterwards.\\ This giant disaster cost approximately 500 million dollars and it was a huge loss for ESA and CNES.  \\
The reason for the failure was an error that could have easily been avoided. Luckily the rocket was unmanned, but this kind of error could happen in any other space rocket. This launch failure is acknowledged as one of the most expensive failures in history. \\
The failure was caused by a bug in the Inertial Reference System (SRI). The SRI system is used to determine the orientation of the rocket, which is also known as the horizontal bias or the BH value. The error occured when a 64-bit floating point number, representing the horizontal velocity, was converted to a 16-bit signed integer, without any exception handling. This code was written in Ada, a language that other Hardware Description Languages (HDLs) have later been based on.\\ As the rockets velocity increased the 64-bit floating point number became larger than what can fit into a 16-bit signed integer and therefore causing an overflow. The SRI system misinterpreted this as true flight-data, and to counteract the "wrong" direction, the engines thrusted to change course and thus it was ripped apart by aerodynamics. The backup system, that should take over when errors occur in the main system, was running the exact same code as the main system and therefore it had failed, for the same reasons, just before the main system failed, causing the self-destruct mechanism to activate.\\

This insident becomes even more horrible when discovered that the BH value, which was the cause of the failure, was not neseccary after launch. The code had been reused from the Ariane 4 rocket, which required the value after launch, but the Ariane 5 rocket did not. The code that could have handled these problems had been disabled due to performance issues on Ariane 4 and had not been reapplied on Ariane 5. Also, Ariane 4 was launched with a less steep trajectory than Ariane 5, and therefore it did not overflow the BH value. However, since Ariane 5 ascent to space faster, it was highly probable that the BH value would overflow. If anyone had taken a look at this, taken the new rocket into account, this massive failure might have been avoided.
\subsection{Therac-25 failure}
In the 80's the company Atomic Energy of Canada Limited (AECL) manufactured a revolutionary radiation therapy machine, the Therac-25\cite{Leveson1993}, which could provide two different kinds of treatment. At that time, hospitals would typically have two different machines for the two different treatments that the Therac-25 machine could provide in one machine. The two treatments consisted of a beam of low-energy electrons which used scanning magnets to spread the electron beam, and a beam of higher-energy X-Ray photons which worked by rotating four components into the beam. The Therac-25 was build based on the previous Therac-20 and Therac-6 and some code from the Therac-20 was reused in the Therac-25. Unfortunately, there were no independent protective circuits for monitoring the electron beam or any interlocks to ensure safety with the Therac-25, which had been in the previous versions. AECL put more faith on software reliability than on hardware. \\ After the Therac-25 had been operational for a couple of years on several different hospitals, a series of incidents happened where patients were exposed to too much radiation and that led to six patients being seriously injured or killed. Friz Hager, the staff physicist at East Texas Cancer Center, tried to reproduce the errors they had experienced on the Therac-25, and was successful. When the user selected the X-Ray mode on the Therac-25, the machine began setting up for high-powered X-rays, which took about 8 seconds. If the user switched to Electron mode before the machine finished setting up for X-ray mode, i.e within 8 seconds, the turntable would not switch to the correct position causing an enormous amount of radiation to reach the patient.\\
After solving the problem and releasing a new version of the Therac-25, another problem emerged where a patient was overdosed. This time it turned out to be a counter overflow. If a command was sent at the exact moment the counter overflowed, the machine would not set op properly and again, resulting in an overdose of radiation for the patient. \\
After this incident with the Therac-25, it was found that some of the same software problems was found in Therac-20, but due to the hardware precautions on the Therac-20, the problems never occurred.
\\ This example shows how important it is for critical systems to be well designed as well as well tested or verified.

\subsection{The patriot misile failure}
During the Persian Gulf war on February 25, 1991, an American Patriot Misile failed to intercept an incomming Iraqi Scud misile which caused the Scud to hit an American Army barracks which injured around 100 people and killed 28 soldiers. The Patriot Misile failed due to an error when converting an interger, representing time since last boot, to a real number using a 24-bit register. As time since last boot increased, the chopping error increased due to the fact that the conversion was fixed to a 24-bit range. At the time of the incident, the Patriod Misile Battery had been up for approximately 100 hours, which caused the chopping inaccuracy to be around 0.34 seconds. The Scud travels at around 1.676 meters per second, and therefore in the 0.34 seconds it travels more than a half kilometer. The consequences of this restriction in  registers caused the precision to be inacurate and so the Patriot Misile did not struck the Scud as it was meant to.


%Intels-division bug \\
%Toyota bremse-fejl\\
\section{Learning goals}
The learning goals accepted for this project are:
\begin{itemize}
\item Reflect on the set of SME expressible problems, that are verifiable with FDR4.
\item Reason about efficient code transformation from an executable format to a verifiable format
\item Reason about design choices and their consequences for execution performance.
\item Demonstrate efficient constraint transfer from SME to FDR4.
\item Reason about SME program size and time to verification.
\item Reflect on the generality of a generic verification template.
\item Disseminate project results to a professional audience.
\end{itemize}
\chapter{Related work}
The concepts of formal verification was first expressed in 1954 when Martin Davis created the first computer generated mathematical proof that the product of two even numbers, is even. First-order theorem provers were applied to verification problems in Pascal, Ada and Java, in the late 1960s.
At Stanford, in 1972, Sir Robin Milner had success building the original LCF system for proof checking and his work in automated reasoning have been the foundation for a lot of other theorem provers, like the proof assistant HOL (Higher Order Logic) by Mike Gordon, which was originally developed for reasoning about hardware. The formal proof management system Coq is a descendent of LCF. \\
Also in 1972, Robert S. Boyer and J. Strother Moore was successful in building a machine-based prover, called Nqthm which became the basis for ACL2 which is a programming language and a theorem prover. Theorem provers have proved very valuable over the time, but one problem with them was, that if they found a problem in a theorem, they could not tell why it could not prove the theorem. It was not possible to create a counter example or any other explanation as to why it was not possible to prove this theorem. \\\\
In 1967, Robert W. Floyd was published with the paper \textit{Assigning meaning to programs}\cite{Floyd1967}. Floyd provided a basis for the formal definitions of the meaning of programs which can be used for proving correctness, equivalence and termination. By using flowcharts, he argued that when a command is reached, all previous commands will have been true as well.\\ C.A.R Hoare was inspired by Floyd and in 1969 his paper \textit{An axiomtic basis for computer programming}\cite{Hoare1969} was published. The logic he presented there (later known as \textit{Hoare logic}), was build on Floyd's ideas and proposed the notation \textit{Partial correctness specification}; $\{P\} C \{Q\}$. Here, $C$ is a command and $P$ and $Q$ are conditions on the program variables in $C$. Hoare showed that whenever $C$ is executed in a state that satisfies the condition $P$, and if the execution terminates, then the state that $C$ terminates in, will satisfy $Q$. Hoares logic have been the basis of a lot of different formal languages and have contributed to the continuous work on formal verification. \\
Since the original Hoares logic was not originially thought as to model concurrent programs, L. Lamport extended Hoare's logic in his paper \textit{The 'Hoare logic' of concurrent programs}\cite{Lamport1980} in 1980. Here, he discuss why Hoare's logic, as proposed by C.A.R Hoare, does not work for concurrent programs and proposes a "generalized Hoare's logic" that takes concurrency into account. \\\\
In 1978 Hoares paper \textit{Communicating Sequential Processes} was published and with it, CSP was born. It have been widely used in many different types of work and have also been expanded since Hoare initially described it in 1978\cite{Abdallah2005}. The first version of CSP was a simple programming language that had quite a different syntax than todays CSP. In 1984, Brookes, Hoare and Roscoe published their continued work on CSP with the paper \textit{A Theory of Communicating Sequential Processes}\cite{Brookes1984}, and created the modern process algebra it is today. Only a few minor changes have been made to CSP since then, and they are described in Roscoe's \textit{The Theory and Practice of Concurrency}\cite{Roscoe1997}.\\
A number of tools have been created in order to analyse, verify and understand systems written in CSP. Since CSP was mostly a blackboard language and difficult to use on larger scale, different types of machine-readable CSP syntaxes have been created over the years in order to make it easier to use CSP on a larger scale. Most of todays CSP tools use a version of machine-readble CSP called \cspm{} which was created by Scattergood\cite{Scattergood1998}. Scattergood created \cspm{} as a combination of the standard CSP algebra and a functional programming language which provided a better baseline for tools to work with CSP.\\
Here is a subset of the different CSP tools:
\begin{itemize}
\item One of the most known CSP tools is the Failure-Divergence Refinement tool (FDR), build by Formal Systems (Europe) Ltd., and is currently at version 4.2.3\cite{fdr}. FDR4 is a refinement checker and the newer version of FDR is able to run in parallel as well as do state compression in order to avoid a very large state space. FDR only work on finite-state processes.
\item ProBE (Process Behaviour Explorer)\cite{probe} is a tool to animate CSP in order to explore the state space of CSP processes. It can handle infinite state and is based on the same \cspm{} version as FDR4 is. ProBE was also created by Formal Systems (Europe) Ltd and ProBE is integrated into the current version of FDR4.
\item At Adelaide University, The Adelaide Refinement Checker (ARC)\cite{Parashkevov1996} was created as an automatic verification tool for CSP. It uses Ordered Binary Decision Diagrams (OBDDs) to represent the internal representation of data structures. This lessen the state explosion problem that other model checker tools have had. \todo{which language does it use?}
\item The ProB project\cite{ProB}\cite{Leuschel2003} was originally created as an animation and model checker tool for the B-Method\cite{Abrial1988} but it also supports other languages like Z and \cspm{}. Newer versions of ProB can do refinement checking of \cspm{} scripts but does not have the full functionality that FDR does. \todo{research this more}
\item J. Sun, Y.Liu, J.Dong et al. presented the Process Analysis Toolkit (PAT) in their 2009 paper\cite{Sun2009}. PAT is a CSP analysis tool that can perform Linear Temporal Logic (LTL) model checking, refinement checking and simulation of CSP processes. \todo{research this more}
\item CSP-Prover\cite{Isobe2005} is a theorem prover for CSP and based on the theorem prover Isabelle. It is an entirely different way to check programs than model checking. It attempts to prove some general results based on specific theory. It is better at proving general results where model checkers are better at proving combinatorial problems. \todo{make this more clear}
\end{itemize}
The programming language Occam\cite{Occam1995}, which was first released in 1983, is a concurrent programming language that builds on the CSP process algebra. Occam was continuouly in development during the years and the Kent Retargetable occam Compiler (KRoC) team at Kent University created the Occam-$\pi$\cite{UniveristyofKent} variant of the Occam programming language. It is a version that extends the ideas of CSP in the original Occam language but adding mobility features from pi-calculus. In the paper \textit{The symbiosis of concurrency and verification: teaching and case studies}\cite{Pedersen2018} Pedersen and Welch uses Occam-$\pi$ along with \cspm in order to reason about the logic behind \cspm and FDR. By using an executable language like Occam-$\pi$ which is based on the concurrency model of CSP it becomes easier to understand the logic of \cspm and thereby verify the program with FDR4.\\\\
SPIN\cite{spin} is a verification tool that uses process interactions to prove correctness for a system. The systems are described in the formal language \texttt{PROMELA}(PROcess MEta LAnguage)\cite{Holzmann1991} and the correctness properties are spcified in Linear Temporal Logic (LTL)\cite{Pnueli1977}. In the paper \textit{Reasoning About Infinite Computations}\cite{Vardi1994}, Vardi and Wolper showed that all LTL formulas can be translated into a B\"uchi automata which SPIN makes use of and thus converting the given LTL into a B\"uchi automaton. Spin performs verification on concurrent software and does not perform verification on hardware circuits. \\
Spin was developed at Bell Labs, starting in 1980. Gerard J. Holzmann gives an introduction to the theoretical foundations, the design and structure and examples of applications in the paper \textit{The model checker SPIN}\cite{Holzmann1997}. SPIN, as well as other model checker tools, have been build on the pioneering work on logic model checking by Clarke and Emerson\cite{Clarke1981}, as well as Sifakis and Queille\cite{Queille1982}. \todo{Should I add more info here? } Vardi and Wolper extended their work with an automata-theoretic approach to automatically verify programs\cite{Vardi1986}.\\\\
Another verification tool was developed as a collaboration between the Department of Information Technology at Uppsala University (UPP) in Sweden and the Department of Computer Science at Aalborg University (AAL) in Denmark. Larsen et al. first proposed the ideas for UPPAAL\cite{Larsen1995} in 1995 and further introduced it in the paper \textit{UPPAAL - a Tool Suite for Automatic Verifcation of Real-Time Systems}\cite{Bengtsson1995}.
UPPAAL is a verification tool for modelling, simulating and verifying real-time systems. It is based on the theory of timed automata\cite{Hopcroft2001}\cite{Alur1990} and the typical systems to gain advantage of UPPAAL are systems where timing aspects are critical and where the communication goes through channels or shared variables.
As other model checkers, UPPAAL have a modelling language, wherein the system is specified, and a query language that is used to specify the properties to check against the system. The query language is a subset of CTL (computational tree logic) that work for real-time systems\cite{Henzinger1994} \cite{Larsen1995}. The model checking is done by checking the state-space by making a reachability analysis. The current version of UPPAAL is called UPPAAL2K and was released in 1999\cite{Amnell2001}. \\\\
In 1981, Edmund M. Clarke and E. Allen Emerson managed to combine temporal logic with the state-space exploration in order to provide the first automated model checking algorithm\cite{Clarke1981}. It was capable of proving properties of programs as well as producing counter examples.
In the mid 1980s it was shown how model checking could be applied to hardware verification. However, it quickly became clear that model checking on hardware was very limited due to the state-space explosion that occurs especially on hardware. \\
Randall Bryant from the CMU electrical engineering department invented ordered Binary decision diagrams (OBDDs). Later on, J. Burch, E. Clarke, K. McMillan et al.\cite{Burch1992} used OBDDs and created \textit{symbolic model checking} which represents the state space symbolically. The symbolic model checking can verify systems with an extremely large number of states and thus creating a solution to the problems of state space explosion.\\
Because of the state-space explosion problem and the increasing complexity of digital electronic circuits, there was a need to be able to model the timing and data flow of a ciruit with a certain amount of abstraction. This became Hardware Description Languages (HDL) \\\\
VHDL (VHSIC Hardware Description Language) was initially ordered by the United States Department of Defence in 1981 to help with the growing problem of hardware life cycles. It is based on the Ada programming language and have been the base Hardware Description language that was used to model hardware. In 1987 it became an IEEE standard, known as VHDL-87. After a major modification in 1993 it was known as VHDL-93. VHDL ... \todo{write something more}

Verilog was published by Gateway Design Automation in 1985 and along side VHDL are the two main HDL's used for modelling circuits. Cadence Design Systems received the rights to Verilog-XL which is the HDL simulator that would end up being the de-facto standard Verilog simulator.


%
%However, VHDL and Verilog share many of the same limitations: neither is suitable for analog or mixed-signal circuit simulation; neither possesses language constructs to describe recursively-generated logic structures. Specialized HDLs (such as Confluence) were introduced with the explicit goal of fixing specific limitations of Verilog and VHDL, though none were ever intended to replace them. (From WIKI)
%(From WIKI): Essential to HDL design is the ability to simulate HDL programs. Simulation allows an HDL description of a design (called a model) to pass design verification, an important milestone that validates the design's intended function (specification) against the code implementation in the HDL description. It also permits architectural exploration. The engineer can experiment with design choices by writing multiple variations of a base design, then comparing their behaviour in simulation. Thus, simulation is critical for successful HDL design.


%Look at functional verification
%
%Look at Property Specification Language
%also look at SVA (two property languages that are derived from LTL) (used for Hardware)
%
%
%HDL include explicit notation for expressing concurrency as well as a notion of time.
%HDLs are used to write executable specifications for hardware.
%Because HDLs can be executed it gives the illusion of programming languages even though it is more of a specification language or modelling language.
%First HDLs in late 60's. C.Gordon Bell and Allan Newells text "Computer Structures" in 1971 - first to give a hdl with lasting effect.
%
%(from
%http://www.techdesignforums.com/practice/guides/formal-verification-guide/) "Equivalence checking has been used for more than a decade to check that RTL and gate-level descriptions of a design represent the same design"
%
%
%
%Take a look at Temporal logic model checking (As it is mentioned in the formal verification - evolution article)
%- Clarke et. al. CMU 1981
%- Sifakis et. al. Grenoble 1982
%and also look at
%Symbolic model checking
%McMillan 1991
%SMV
%
%WRIGHT\cite{Allen1997}\cite{Allen1997a} \todo{It would be worth to read more about this! They have done a bit of the same that I am to do in my thesis with auto generating \cspm}
%is an architecture description language which was developed at Carnegie Mellon University. They can auto generate \cspm code from WRIGHT and from there they can confirm certain properties by using FDR. http://www.cs.cmu.edu/~able/wright/
%
%
%
%Both theorem provers and model checkers have been, and are still, widely used for both software and hardware. There is a third form of formal verification that is also being used more often now. This is equivalence checking, which compares two models of a design and produces an outcome that either shows that they are equal or provides a counter-example to show when they disagree. It is beginning to become common practice for hardware designers to use equvalence checking to compare the design of an optimized digital design and an unoptimized digital design. This way it is possible for the designer to check that the optimizations did not change the functionality of the design.
%





\chapter{Analysis}
\section{SME}

\section{SMEIL}
%\textbf{This is where the theory go. fx. SME and the correlation between that and CSP.}
%CSPm was devised by Bryan Scattergood as a machine-readable dialect of CSP  - se the paper \textit{The Semantics and Implementation of Machine-Readable CSP}\\\\
%" FDR2 is often described as a model checker, but is technically a refinement checker, in that it converts two CSP process expressions into Labelled Transition Systems (LTSs), and then determines whether one of the processes is a refinement of the other within some specified semantic model (traces, failures, or failures/divergence)" (from Wikipedia - se paper \textit{Model-checking CSP - af Roscoe} \\\\
\section{CSP}
Today, Communicating Sequential Processes (CSP) is a process algebra that provides a way to express concurrent systems. By using message passing between processes the language avoids certain problems that arise with the use of e.g shared variables. An essential part of CSP is message passing and the syntax for input is \texttt{X?c}. This represents an input from channel\texttt{X} and an assignment of the input value to the variable \texttt{c}. The output syntax is \texttt{X!c} where the value of the variable \texttt{c} is sent over the output channel \texttt{X}. At first Hoare had defined the message syntax to use the process names, but later on when CSP was developed into a proper process algebra, the syntax changed into using channels in order to be able to have several processes connected via the same channels.   \textbf{do not write too much here. just short explain csp}
\section{\cspm{}}
\cspm is a formal language that combines CSP with a functional programming language in order to make it easier for the programmer to model the systems and then use the code on tools that can animate, verify or similar.
\section{FDR}
FDR (Failures Divergence Refinement) tool is a refinement checker for

In the paper \textit{A primer on model checking}\cite{Ben-ari2010} Mordechai Ben-Ari explains a concurrent problem that he had used for many years, to teach his students about concurrency. ... \textbf{write this when I have read the article again}



\chapter{Generalize specification from program and traces}
\section{Target solution}

\section{Manual translation}
The first step in translating from SMEIL to \cspm is to create a small example and create a manual translation. This ensures that we have a suitable example to test the automatic gode generation, but also gives a good understanding of how the translation could be created and what kind of challenges there will arise from translating from SMEIL to \cspm.
The example \textit{Seven segment example} is an example of modelling a digital clock that consists of 6 different 7-segment displays. A 7-segment display is a display device for displaying decimal numerals. It consist of 7 identical segments which can be lit in different combinations in order to show the Arabic numerals 0 to 9. In the example we model a circuit that receives an input in the form; "seconds after midnight" and from this, calculates and displays the correct hours, minutes and seconds on the displays. Since only one digit can be shown on each 7-segment display it is necessary to separate the actual number into two e.g if the hour is 12 it will be shown as 1 and 2 on two separate displays. \\ The SME example in figure \ref{seven_segments_exaple.sme} inputs a numeral from a source process that is incremented by one for each run. It then sends the input out on the output bus where three different calculating processes receives from. Each process calculates respectively hours, minutes and seconds and separates the number in order to output the result to the two output channels. \\ The \cspm code in figure \ref{seven_segments_exaple.csp} is the handmade translation from the SME file. \\ The translation in \cspm is equivalent to the SME version where each process calculates either the hours, minutes or seconds and separate the result into two digits, one for each output channel.\\
What we need to assert in this example is the number that is sent to the 7-segment displays. A 7 segment display can only represent 0-9 but 4 bits can represent 0-15. This means that we are interested in figuring out if this model can result in an ouput less that 10, in which case, the assertion fails and the model needs to be changed. The assertion used in \cspm checks if the processes refines the \texttt{SKIP} process i.e if the process terminates. This works because of the \texttt{if-then-else} statement that ensures that the process never stops (\texttt{STOP}) if one of the outputs are larger than 10. \\\\
One problem that arises when trying to translate SMEIL to \cspm, is that in SMEIL the input has to be generated by a process, i.e there is no input from file or stdout.
Therefore we create a source process, in this case the \texttt{clock()} process, that does not have any input but, in this case, generates a variable which is saved and incremented by one for each run.
Now, FDR checks all possible inputs and therefore we have to find a solution so that we can figure out the entire input range automatic.
The solution lies in the SMIEL simulator.
When simulating the sme program, it creates a range of all observed inputs on all channels in order to change the general \texttt{int} to e.g \texttt{i4} if all observed inputs are within i4.
Then we can observe and generalize that a process with no input bus will be the source process that generates the input for the circuit.
Due to this generalization, we can translate the output bus of the source process in sme to be the the input channel in \cspm.
That is, the range observed on the output bus in sme will be translated into the range of the input channel in \cspm.
So in reality the source process is not created as an actual process in \cspm and therefore it is also crucial that the source process in sme does not have hidden channels or do calculations that we wish to assert on. \\
In \cspm, one always note the input ranges of channel if the channel carries numerals. However if we simply write \texttt{channel input : int}, then FDR will check for all integers, which will last forever, therefore we wish to create the proper range on the input channel by using the source process in sme.
However for the rest of the circuit, we might need to assert on the input or output of the busses and therefore we do not wish to use the ranges from the sme simulator on other busses than the output bus from the source process.
Since it is necessary to give a range to all channels in \cspm, as mentioned above, the challenge lies in figuring out the correct ranges for these channels. \\

\begin{figure}
\label{seven_segments_exaple.sme}
\caption{Seven segments example in SMEIL}
\end{figure}


\begin{figure}
\label{seven_segments_exaple.csp}
\caption{Seven segments example in \cspm}
\end{figure}


\section{Automated translation}
 \textbf{It is important to mention that the FDR version of the SMEIL program are represented as one clock cycle and therefore we do not have to handle implicit clock cycle issues. we can just translate one-to-one, because FDR models one clock cycle and the input represents all possible input in one clock cycle.}

 \textbf{From my paper - the ANTLR section}
\subsection{ANTLR4}
For the transpiling between SMEIL source code and \cspm{} source code, we decided to use ANTLR4~\cite{antlr} for creating a parser and a lexer. ANTLR4 is a Java-based parser generator library that, based on a grammar, can generate parsers in Java or another target language. ANTLR4 provided a tool that could easily transform the given grammar into a parser and lexer that could immediately be used to transform into \cspm{}.

ANTLR takes a grammar, defined in .g4 (BKNF?) and create the parser and lexer of the files.

A lexical analysis, which is what the lexer does, is a process of converting a string of characters into tokens, which is also called tokenization. Each token represents a lexer rule in the grammer, for instance, if the string is "123" and there is a lexer rule "INT: {0-9}+" which means one or more of 0-9 digits. Then the token would be an INT.

A parser...\todo{write more here}

Currently, only a subset of the SMEIL grammar have been implemented for translating and parts of the grammar have been changed slightly to match the expectations from a simulated SMEIL program, which varies a bit from a non-simulated SMEIL program. An example of this could be that in the original SMEIL grammar a channel declaration only includes an optional range, however, we are expecting a simulated SMEIL program and therefore the simulation would always have generated ranges of observed values for each channel. Therefore, in the grammar, created for ANTLR4, the range for each channel must always be defined.

The ANTRL4 grammar is provided in a filetype called \texttt{.g4} but the structure of the grammar is similar to standard grammar notation.
After running ANTLR4 and generating a parser and a lexer, one can decide to traverse the parse tree itself or use a listener or visitor that ANTLR4 provides. The main difference between the two is that the methods the listener provides are called by the walker object, which ANTLR4 provides, and the visitor methods must call their children explicitly to walk them.
For our implementation, we used the ANTLR4 listener along with Python. When generating a parser and lexer for another target language than Java, the programmer only has to specify this in the ANTLR4 command in the command-line.


\chapter{Experiments and results}
\chapter{Discussion}

\chapter{Conclusion}

\section{Future work}

\newpage
\bibliographystyle{abbrv}
\bibliography{library}
\end{document}
